{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Case_study_2_Fashion_mnist.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitknsingh/Hello-world/blob/master/Case_study_2_Fashion_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOx7aQEsjg1v"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import load_model \n",
        "import os\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Activation "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in keras there are inbuilt datasets ---> fashion mnist datast---> download the dataset \n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_9aTKMHlHAc",
        "outputId": "88b27f5e-0236-4465-9cca-241145ae769a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "metadata": {
        "id": "DlzZvkgClyVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFc3ShAvmIJ0",
        "outputId": "947c9708-5b10-4df1-84c3-5ece468da9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape((60000,28,28,1))\n",
        "X_test = X_test.reshape((10000,28,28,1))"
      ],
      "metadata": {
        "id": "LOAmoH5NmTrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train/255.0 # 0-255 \n",
        "X_test = X_test/255.0"
      ],
      "metadata": {
        "id": "r0AEaxynmXrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_width=28\n",
        "img_height=28\n",
        "\n",
        "input_shape=(img_width,img_height,1)"
      ],
      "metadata": {
        "id": "rp2d4uewtXdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()"
      ],
      "metadata": {
        "id": "ckCN7Pifm3k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv1 = layers.Conv2D(64, (3,3), activation='relu', input_shape=(28,28,1) ) # 64/128/256--> thumb rule \n",
        "conv2 = layers.Conv2D(128, (3,3), activation='relu')\n",
        "max_pool = layers.MaxPooling2D((2,2))\n",
        "\n",
        "flat_layer = layers.Flatten()\n",
        "\n",
        "fc1 = layers.Dense(64, activation='relu')\n",
        "fc2 = layers.Dense(64, activation='relu')\n",
        "\n",
        "output = layers.Dense(10, 'softmax')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CDueZxOntMfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(conv1)\n",
        "model.add(conv2)\n",
        "model.add(max_pool)\n",
        "model.add(flat_layer)\n",
        "model.add(fc1)\n",
        "model.add(fc2)\n",
        "model.add(output)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvkn6K4NvELI",
        "outputId": "d59eb69a-d205-4e1c-cc16-be5130043820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 64)        640       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 24, 24, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 12, 12, 128)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 18432)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                1179712   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,259,018\n",
            "Trainable params: 1,259,018\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "YR783vBXvUwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train , epochs = 200, batch_size = 64, validation_split = 0.2 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ldPrYgngvk2l",
        "outputId": "379c3050-791f-4bbf-ad59-1a8ce9c5993a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "750/750 [==============================] - 26s 17ms/step - loss: 0.4451 - accuracy: 0.8428 - val_loss: 0.3001 - val_accuracy: 0.8937\n",
            "Epoch 2/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.2717 - accuracy: 0.9005 - val_loss: 0.2702 - val_accuracy: 0.9013\n",
            "Epoch 3/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.2165 - accuracy: 0.9200 - val_loss: 0.2314 - val_accuracy: 0.9147\n",
            "Epoch 4/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.1760 - accuracy: 0.9342 - val_loss: 0.2415 - val_accuracy: 0.9147\n",
            "Epoch 5/200\n",
            "750/750 [==============================] - 12s 16ms/step - loss: 0.1430 - accuracy: 0.9471 - val_loss: 0.2361 - val_accuracy: 0.9181\n",
            "Epoch 6/200\n",
            "750/750 [==============================] - 12s 16ms/step - loss: 0.1155 - accuracy: 0.9575 - val_loss: 0.2637 - val_accuracy: 0.9119\n",
            "Epoch 7/200\n",
            "750/750 [==============================] - 12s 16ms/step - loss: 0.0917 - accuracy: 0.9659 - val_loss: 0.2691 - val_accuracy: 0.9216\n",
            "Epoch 8/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0721 - accuracy: 0.9740 - val_loss: 0.2955 - val_accuracy: 0.9153\n",
            "Epoch 9/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 0.0558 - accuracy: 0.9798 - val_loss: 0.3427 - val_accuracy: 0.9179\n",
            "Epoch 10/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 0.0475 - accuracy: 0.9829 - val_loss: 0.3432 - val_accuracy: 0.9182\n",
            "Epoch 11/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 0.0409 - accuracy: 0.9858 - val_loss: 0.3554 - val_accuracy: 0.9194\n",
            "Epoch 12/200\n",
            "750/750 [==============================] - 12s 16ms/step - loss: 0.0328 - accuracy: 0.9883 - val_loss: 0.4129 - val_accuracy: 0.9137\n",
            "Epoch 13/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0290 - accuracy: 0.9901 - val_loss: 0.4053 - val_accuracy: 0.9151\n",
            "Epoch 14/200\n",
            "750/750 [==============================] - 12s 16ms/step - loss: 0.0263 - accuracy: 0.9910 - val_loss: 0.4250 - val_accuracy: 0.9179\n",
            "Epoch 15/200\n",
            "750/750 [==============================] - 12s 16ms/step - loss: 0.0268 - accuracy: 0.9906 - val_loss: 0.4599 - val_accuracy: 0.9162\n",
            "Epoch 16/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0213 - accuracy: 0.9924 - val_loss: 0.4919 - val_accuracy: 0.9174\n",
            "Epoch 17/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0214 - accuracy: 0.9928 - val_loss: 0.5513 - val_accuracy: 0.9097\n",
            "Epoch 18/200\n",
            "750/750 [==============================] - 12s 16ms/step - loss: 0.0201 - accuracy: 0.9934 - val_loss: 0.5513 - val_accuracy: 0.9126\n",
            "Epoch 19/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 0.0192 - accuracy: 0.9932 - val_loss: 0.5518 - val_accuracy: 0.9150\n",
            "Epoch 20/200\n",
            "750/750 [==============================] - 12s 16ms/step - loss: 0.0156 - accuracy: 0.9948 - val_loss: 0.5477 - val_accuracy: 0.9146\n",
            "Epoch 21/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0168 - accuracy: 0.9949 - val_loss: 0.5732 - val_accuracy: 0.9176\n",
            "Epoch 22/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0181 - accuracy: 0.9942 - val_loss: 0.5493 - val_accuracy: 0.9182\n",
            "Epoch 23/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 0.0168 - accuracy: 0.9946 - val_loss: 0.5842 - val_accuracy: 0.9122\n",
            "Epoch 24/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 0.0156 - accuracy: 0.9945 - val_loss: 0.5871 - val_accuracy: 0.9140\n",
            "Epoch 25/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 0.6274 - val_accuracy: 0.9114\n",
            "Epoch 26/200\n",
            "750/750 [==============================] - 12s 16ms/step - loss: 0.0144 - accuracy: 0.9952 - val_loss: 0.6336 - val_accuracy: 0.9170\n",
            "Epoch 27/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0183 - accuracy: 0.9938 - val_loss: 0.6108 - val_accuracy: 0.9145\n",
            "Epoch 28/200\n",
            "750/750 [==============================] - 12s 16ms/step - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.6435 - val_accuracy: 0.9181\n",
            "Epoch 29/200\n",
            "750/750 [==============================] - 12s 16ms/step - loss: 0.0084 - accuracy: 0.9970 - val_loss: 0.6747 - val_accuracy: 0.9185\n",
            "Epoch 30/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0161 - accuracy: 0.9946 - val_loss: 0.6340 - val_accuracy: 0.9165\n",
            "Epoch 31/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0135 - accuracy: 0.9952 - val_loss: 0.6735 - val_accuracy: 0.9139\n",
            "Epoch 32/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.6700 - val_accuracy: 0.9153\n",
            "Epoch 33/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0085 - accuracy: 0.9971 - val_loss: 0.7232 - val_accuracy: 0.9212\n",
            "Epoch 34/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0129 - accuracy: 0.9956 - val_loss: 0.6789 - val_accuracy: 0.9194\n",
            "Epoch 35/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.7368 - val_accuracy: 0.9191\n",
            "Epoch 36/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.6953 - val_accuracy: 0.9182\n",
            "Epoch 37/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0106 - accuracy: 0.9964 - val_loss: 0.7658 - val_accuracy: 0.9120\n",
            "Epoch 38/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0142 - accuracy: 0.9958 - val_loss: 0.7325 - val_accuracy: 0.9191\n",
            "Epoch 39/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.7581 - val_accuracy: 0.9147\n",
            "Epoch 40/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.8350 - val_accuracy: 0.9146\n",
            "Epoch 41/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0069 - accuracy: 0.9977 - val_loss: 0.7896 - val_accuracy: 0.9133\n",
            "Epoch 42/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0159 - accuracy: 0.9952 - val_loss: 0.7868 - val_accuracy: 0.9176\n",
            "Epoch 43/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.7961 - val_accuracy: 0.9165\n",
            "Epoch 44/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0087 - accuracy: 0.9974 - val_loss: 0.7852 - val_accuracy: 0.9147\n",
            "Epoch 45/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0121 - accuracy: 0.9962 - val_loss: 0.7844 - val_accuracy: 0.9142\n",
            "Epoch 46/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0103 - accuracy: 0.9965 - val_loss: 0.9047 - val_accuracy: 0.9106\n",
            "Epoch 47/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0116 - accuracy: 0.9965 - val_loss: 0.7661 - val_accuracy: 0.9164\n",
            "Epoch 48/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.7980 - val_accuracy: 0.9166\n",
            "Epoch 49/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.8744 - val_accuracy: 0.9121\n",
            "Epoch 50/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0113 - accuracy: 0.9965 - val_loss: 0.8661 - val_accuracy: 0.9148\n",
            "Epoch 51/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0091 - accuracy: 0.9965 - val_loss: 0.7923 - val_accuracy: 0.9126\n",
            "Epoch 52/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.8631 - val_accuracy: 0.9203\n",
            "Epoch 53/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0088 - accuracy: 0.9978 - val_loss: 0.8986 - val_accuracy: 0.9129\n",
            "Epoch 54/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0133 - accuracy: 0.9962 - val_loss: 0.8503 - val_accuracy: 0.9146\n",
            "Epoch 55/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.8053 - val_accuracy: 0.9153\n",
            "Epoch 56/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.9041 - val_accuracy: 0.9168\n",
            "Epoch 57/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.8846 - val_accuracy: 0.9146\n",
            "Epoch 58/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 0.0108 - accuracy: 0.9968 - val_loss: 0.9052 - val_accuracy: 0.9110\n",
            "Epoch 59/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.9025 - val_accuracy: 0.9100\n",
            "Epoch 60/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.9159 - val_accuracy: 0.9191\n",
            "Epoch 61/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 0.9166 - val_accuracy: 0.9135\n",
            "Epoch 62/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.9335 - val_accuracy: 0.9129\n",
            "Epoch 63/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.8960 - val_accuracy: 0.9098\n",
            "Epoch 64/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.8787 - val_accuracy: 0.9183\n",
            "Epoch 65/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.8647 - val_accuracy: 0.9178\n",
            "Epoch 66/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.8626 - val_accuracy: 0.9186\n",
            "Epoch 67/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0061 - accuracy: 0.9981 - val_loss: 0.9144 - val_accuracy: 0.9129\n",
            "Epoch 68/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.9809 - val_accuracy: 0.9218\n",
            "Epoch 69/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.8032 - val_accuracy: 0.9180\n",
            "Epoch 70/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.8294 - val_accuracy: 0.9160\n",
            "Epoch 71/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.9569 - val_accuracy: 0.9159\n",
            "Epoch 72/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 1.0372 - val_accuracy: 0.9179\n",
            "Epoch 73/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0101 - accuracy: 0.9972 - val_loss: 1.0061 - val_accuracy: 0.9143\n",
            "Epoch 74/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.9956 - val_accuracy: 0.9186\n",
            "Epoch 75/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.9937 - val_accuracy: 0.9165\n",
            "Epoch 76/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 0.0092 - accuracy: 0.9973 - val_loss: 1.0086 - val_accuracy: 0.9135\n",
            "Epoch 77/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.8680 - val_accuracy: 0.9189\n",
            "Epoch 78/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.3292e-04 - accuracy: 0.9998 - val_loss: 0.9433 - val_accuracy: 0.9223\n",
            "Epoch 79/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 4.4788e-05 - accuracy: 1.0000 - val_loss: 0.9432 - val_accuracy: 0.9234\n",
            "Epoch 80/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 1.4963e-05 - accuracy: 1.0000 - val_loss: 0.9631 - val_accuracy: 0.9226\n",
            "Epoch 81/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 6.7352e-06 - accuracy: 1.0000 - val_loss: 0.9759 - val_accuracy: 0.9234\n",
            "Epoch 82/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 4.7982e-06 - accuracy: 1.0000 - val_loss: 0.9891 - val_accuracy: 0.9237\n",
            "Epoch 83/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 3.5355e-06 - accuracy: 1.0000 - val_loss: 1.0026 - val_accuracy: 0.9238\n",
            "Epoch 84/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 2.6156e-06 - accuracy: 1.0000 - val_loss: 1.0172 - val_accuracy: 0.9245\n",
            "Epoch 85/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 1.9193e-06 - accuracy: 1.0000 - val_loss: 1.0319 - val_accuracy: 0.9246\n",
            "Epoch 86/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 1.4071e-06 - accuracy: 1.0000 - val_loss: 1.0472 - val_accuracy: 0.9253\n",
            "Epoch 87/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 1.0152e-06 - accuracy: 1.0000 - val_loss: 1.0642 - val_accuracy: 0.9252\n",
            "Epoch 88/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 7.2848e-07 - accuracy: 1.0000 - val_loss: 1.0816 - val_accuracy: 0.9253\n",
            "Epoch 89/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 5.1668e-07 - accuracy: 1.0000 - val_loss: 1.1011 - val_accuracy: 0.9248\n",
            "Epoch 90/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 3.6441e-07 - accuracy: 1.0000 - val_loss: 1.1191 - val_accuracy: 0.9248\n",
            "Epoch 91/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 2.5619e-07 - accuracy: 1.0000 - val_loss: 1.1387 - val_accuracy: 0.9248\n",
            "Epoch 92/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 1.8009e-07 - accuracy: 1.0000 - val_loss: 1.1589 - val_accuracy: 0.9253\n",
            "Epoch 93/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 1.2490e-07 - accuracy: 1.0000 - val_loss: 1.1796 - val_accuracy: 0.9255\n",
            "Epoch 94/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 8.6069e-08 - accuracy: 1.0000 - val_loss: 1.2016 - val_accuracy: 0.9254\n",
            "Epoch 95/200\n",
            "750/750 [==============================] - 12s 17ms/step - loss: 6.0240e-08 - accuracy: 1.0000 - val_loss: 1.2220 - val_accuracy: 0.9257\n",
            "Epoch 96/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 4.1974e-08 - accuracy: 1.0000 - val_loss: 1.2429 - val_accuracy: 0.9256\n",
            "Epoch 97/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 2.9167e-08 - accuracy: 1.0000 - val_loss: 1.2653 - val_accuracy: 0.9257\n",
            "Epoch 98/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 2.0598e-08 - accuracy: 1.0000 - val_loss: 1.2869 - val_accuracy: 0.9256\n",
            "Epoch 99/200\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 1.4571e-08 - accuracy: 1.0000 - val_loss: 1.3072 - val_accuracy: 0.9258\n",
            "Epoch 100/200\n",
            "609/750 [=======================>......] - ETA: 2s - loss: 1.0112e-08 - accuracy: 1.0000"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7a58e583bcbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1219\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \"\"\"\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1103\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_finalize_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how do you save the model trained : deploy this model anywhere we want to us  \n",
        "import tempfile\n",
        "model_directory = tempfile.gettempdir()\n",
        "print(model_directory)\n",
        "version =1 \n",
        "export_path = os.path.join(model_directory,str(version))\n",
        "print(export_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr9fX7eTxfxo",
        "outputId": "cb7154f2-8d32-4ffe-f515-5fe40f3a90e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp\n",
            "/tmp/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isdir(export_path):\n",
        "  print('\\n Already the path contains a saved model, cleaning up \\n')\n",
        "  !rm -r {export_path}\n",
        "model.save(export_path, save_format = \"tf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v1zTvJa2XXj",
        "outputId": "633b1cbc-acde-4446-ea9a-ab67b60dc05f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/1/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in tensorflow keras the model is saved as a pb file "
      ],
      "metadata": {
        "id": "OEDtELkS2SF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# suppose you have saved your model in your server---> I am a client of yours \n",
        "# How do I access your model ? \n",
        "# i want to infer from a website your model \n",
        "\n",
        "# Basically we will have to create a server (of the model)"
      ],
      "metadata": {
        "id": "B6egxynN3Pa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!saved_model_cli show --dir {export_path} --all`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LX1Mmw538UI",
        "outputId": "72947715-849c-4172-db86-46c46e59e65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['__saved_model_init_op']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['__saved_model_init_op'] tensor_info:\n",
            "        dtype: DT_INVALID\n",
            "        shape: unknown_rank\n",
            "        name: NoOp\n",
            "  Method name is: \n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['conv2d_input'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 28, 28, 1)\n",
            "        name: serving_default_conv2d_input:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['dense_2'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 10)\n",
            "        name: StatefulPartitionedCall:0\n",
            "  Method name is: tensorflow/serving/predict\n",
            "\n",
            "Defined Functions:\n",
            "  Function Name: '__call__'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "\n",
            "  Function Name: '_default_save_signature'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
            "\n",
            "  Function Name: 'call_and_return_all_conditional_losses'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "!apt update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VTqEnvf4OfG",
        "outputId": "eb1934e6-33c1-4c3f-d5e8-645dc9aa2c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2943  100  2943    0     0   4392      0 --:--:-- --:--:-- --:--:--  4385\n",
            "OK\n",
            "Get:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:4 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,819 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [933 kB]\n",
            "Get:16 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease [3,012 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,228 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,898 kB]\n",
            "Get:19 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.9 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [691 kB]\n",
            "Get:23 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 Packages [339 B]\n",
            "Get:24 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,461 kB]\n",
            "Get:25 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,450 kB]\n",
            "Get:26 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server-universal amd64 Packages [348 B]\n",
            "Fetched 12.8 MB in 3s (3,689 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "58 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we are going to serve our model using package tensorflow serving ---> "
      ],
      "metadata": {
        "id": "o4l1Bu0F4ZQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tensorflow-model-server"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fN_xSnU5Tem",
        "outputId": "1275b676-6b31-461a-faac-4e60b67fcc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tensorflow-model-server\n",
            "0 upgraded, 1 newly installed, 0 to remove and 58 not upgraded.\n",
            "Need to get 335 MB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 tensorflow-model-server all 2.7.0 [335 MB]\n",
            "Fetched 335 MB in 11s (29.6 MB/s)\n",
            "Selecting previously unselected package tensorflow-model-server.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../tensorflow-model-server_2.7.0_all.deb ...\n",
            "Unpacking tensorflow-model-server (2.7.0) ...\n",
            "Setting up tensorflow-model-server (2.7.0) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"Model_Directory\"] = model_directory"
      ],
      "metadata": {
        "id": "Z33ag0qJ6u-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --bg \n",
        "\n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port=8501 \\\n",
        "  --model_name=Fashion_MNIST_Model \\\n",
        "  --model_base_path=\"${Model_Directory}\" >server.log 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km-QICtW55zc",
        "outputId": "b20fc666-70f6-44ad-dcb8-8556a5cc5fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting job # 0 in a separate thread.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail server.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGLmbQhH9dlP",
        "outputId": "f7e66e7f-7826-40a0-a12a-daa0c5e2520b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[warn] getaddrinfo: address family for nodename not supported\n",
            "[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json \n",
        "\n",
        "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": X_test[4:7].tolist()}) #assignment \n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iA96F4R9qVZ",
        "outputId": "bdc375e3-a007-4bd8-ee63-df58991ba590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"signature_name\": \"serving_default\", \"instances\": [[[[0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.00392156862745098], [0.00392156862745098], [0.0], [0.0], [0.0], [0.0], [0.2235294117647059], [0.2627450980392157], [0.28627450980392155], [0.2980392156862745], [0.2980392156862745], [0.3254901960784314], [0.24313725490196078], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.00392156862745098], [0.00392156862745098], [0.00392156862745098], [0.0], [0.0], [0.050980392156862744], [0.30980392156862746], [0.5019607843137255], [0.788235294117647], [0.6352941176470588], [0.6313725490196078], [0.6784313725490196], [0.7529411764705882], [0.6745098039215687], [0.7098039215686275], [0.7215686274509804], [0.4235294117647059], [0.11764705882352941], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.4], [0.5450980392156862], [0.5568627450980392], [0.403921568627451], [0.45098039215686275], [0.6352941176470588], [0.6039215686274509], [0.6470588235294118], [0.6], [0.5450980392156862], [0.5058823529411764], [0.5882352941176471], [0.5411764705882353], [0.6705882352941176], [0.6313725490196078], [0.10196078431372549], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.41568627450980394], [0.48627450980392156], [0.4235294117647059], [0.403921568627451], [0.41568627450980394], [0.36470588235294116], [0.39215686274509803], [0.7058823529411765], [0.611764705882353], [0.5764705882352941], [0.5411764705882353], [0.3333333333333333], [0.615686274509804], [0.4470588235294118], [0.48627450980392156], [0.6039215686274509], [0.615686274509804], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.11372549019607843], [0.5254901960784314], [0.396078431372549], [0.44313725490196076], [0.4235294117647059], [0.3803921568627451], [0.4549019607843137], [0.3176470588235294], [0.5725490196078431], [0.7176470588235294], [0.6431372549019608], [0.43529411764705883], [0.5725490196078431], [0.5137254901960784], [0.47843137254901963], [0.5176470588235295], [0.5686274509803921], [0.6627450980392157], [0.36470588235294116], [0.0], [0.00392156862745098], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.2549019607843137], [0.5137254901960784], [0.4117647058823529], [0.396078431372549], [0.4235294117647059], [0.39215686274509803], [0.40784313725490196], [0.3803921568627451], [0.2901960784313726], [0.807843137254902], [0.6823529411764706], [0.45098039215686275], [0.5882352941176471], [0.4235294117647059], [0.4666666666666667], [0.5725490196078431], [0.596078431372549], [0.6352941176470588], [0.5529411764705883], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.4235294117647059], [0.4823529411764706], [0.4392156862745098], [0.41568627450980394], [0.3843137254901961], [0.39215686274509803], [0.396078431372549], [0.43529411764705883], [0.2823529411764706], [0.5333333333333333], [0.5176470588235295], [0.4392156862745098], [0.45098039215686275], [0.42745098039215684], [0.5568627450980392], [0.5882352941176471], [0.6274509803921569], [0.6352941176470588], [0.7647058823529411], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.5294117647058824], [0.47843137254901963], [0.4666666666666667], [0.4392156862745098], [0.3254901960784314], [0.36470588235294116], [0.3803921568627451], [0.41568627450980394], [0.45098039215686275], [0.3568627450980392], [0.42745098039215684], [0.3254901960784314], [0.42745098039215684], [0.49019607843137253], [0.6470588235294118], [0.5490196078431373], [0.7568627450980392], [0.6274509803921569], [0.6901960784313725], [0.023529411764705882], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.09019607843137255], [0.5294117647058824], [0.5176470588235295], [0.5843137254901961], [0.40784313725490196], [0.3058823529411765], [0.3764705882352941], [0.3803921568627451], [0.403921568627451], [0.4235294117647059], [0.4235294117647059], [0.45098039215686275], [0.32941176470588235], [0.4470588235294118], [0.5843137254901961], [0.6196078431372549], [0.5764705882352941], [0.8196078431372549], [0.6274509803921569], [0.6980392156862745], [0.20392156862745098], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.2235294117647059], [0.48627450980392156], [0.5137254901960784], [0.6274509803921569], [0.403921568627451], [0.3764705882352941], [0.396078431372549], [0.42745098039215684], [0.42745098039215684], [0.43529411764705883], [0.4235294117647059], [0.4470588235294118], [0.41568627450980394], [0.44313725490196076], [0.611764705882353], [0.6392156862745098], [0.611764705882353], [0.7686274509803922], [0.6549019607843137], [0.6823529411764706], [0.3333333333333333], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.33725490196078434], [0.4549019607843137], [0.49411764705882355], [0.6274509803921569], [0.5176470588235295], [0.4], [0.3764705882352941], [0.40784313725490196], [0.4196078431372549], [0.3843137254901961], [0.36470588235294116], [0.4823529411764706], [0.4549019607843137], [0.4392156862745098], [0.5843137254901961], [0.6274509803921569], [0.7098039215686275], [0.7294117647058823], [0.6352941176470588], [0.6352941176470588], [0.4823529411764706], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.4392156862745098], [0.4470588235294118], [0.4392156862745098], [0.6549019607843137], [0.5725490196078431], [0.39215686274509803], [0.39215686274509803], [0.396078431372549], [0.4196078431372549], [0.3764705882352941], [0.39215686274509803], [0.49411764705882355], [0.403921568627451], [0.47058823529411764], [0.5529411764705883], [0.6196078431372549], [0.6549019607843137], [0.7333333333333333], [0.5764705882352941], [0.5803921568627451], [0.6666666666666666], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.48627450980392156], [0.4627450980392157], [0.396078431372549], [0.7725490196078432], [0.34901960784313724], [0.396078431372549], [0.39215686274509803], [0.3764705882352941], [0.4235294117647059], [0.403921568627451], [0.4235294117647059], [0.47843137254901963], [0.4196078431372549], [0.4980392156862745], [0.5450980392156862], [0.5882352941176471], [0.4666666666666667], [0.7686274509803922], [0.5686274509803921], [0.5568627450980392], [0.7019607843137254], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.5137254901960784], [0.45098039215686275], [0.3803921568627451], [0.7764705882352941], [0.1843137254901961], [0.4235294117647059], [0.3764705882352941], [0.3764705882352941], [0.41568627450980394], [0.4666666666666667], [0.4], [0.47058823529411764], [0.403921568627451], [0.4823529411764706], [0.5490196078431373], [0.5882352941176471], [0.3176470588235294], [0.807843137254902], [0.5725490196078431], [0.5294117647058824], [0.7607843137254902], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.01568627450980392], [0.5333333333333333], [0.4627450980392157], [0.3843137254901961], [0.7568627450980392], [0.08235294117647059], [0.42745098039215684], [0.3764705882352941], [0.41568627450980394], [0.4], [0.5058823529411764], [0.39215686274509803], [0.4666666666666667], [0.4], [0.4627450980392157], [0.5529411764705883], [0.6], [0.17647058823529413], [0.8470588235294118], [0.5803921568627451], [0.5450980392156862], [0.803921568627451], [0.047058823529411764], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.09411764705882353], [0.5372549019607843], [0.4588235294117647], [0.396078431372549], [0.7333333333333333], [0.09803921568627451], [0.44313725490196076], [0.3607843137254902], [0.4392156862745098], [0.3686274509803922], [0.47058823529411764], [0.4117647058823529], [0.4980392156862745], [0.3803921568627451], [0.45098039215686275], [0.5568627450980392], [0.5882352941176471], [0.07450980392156863], [0.8352941176470589], [0.5803921568627451], [0.5137254901960784], [0.8], [0.1411764705882353], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.1568627450980392], [0.5529411764705883], [0.42745098039215684], [0.4588235294117647], [0.6196078431372549], [0.047058823529411764], [0.48627450980392156], [0.35294117647058826], [0.4549019607843137], [0.3764705882352941], [0.4588235294117647], [0.44313725490196076], [0.5333333333333333], [0.3686274509803922], [0.43529411764705883], [0.5764705882352941], [0.6392156862745098], [0.12156862745098039], [0.7490196078431373], [0.5725490196078431], [0.5254901960784314], [0.807843137254902], [0.22745098039215686], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.15294117647058825], [0.5058823529411764], [0.4], [0.5764705882352941], [0.4666666666666667], [0.0], [0.47058823529411764], [0.35294117647058826], [0.4666666666666667], [0.396078431372549], [0.4549019607843137], [0.41568627450980394], [0.4980392156862745], [0.4], [0.4470588235294118], [0.5725490196078431], [0.7058823529411765], [0.0784313725490196], [0.5725490196078431], [0.6235294117647059], [0.5058823529411764], [0.8], [0.27450980392156865], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.22745098039215686], [0.49411764705882355], [0.43529411764705883], [0.6352941176470588], [0.396078431372549], [0.08235294117647059], [0.5176470588235295], [0.34901960784313724], [0.4823529411764706], [0.4235294117647059], [0.41568627450980394], [0.4], [0.49411764705882355], [0.43529411764705883], [0.4549019607843137], [0.5529411764705883], [0.6980392156862745], [0.19607843137254902], [0.4392156862745098], [0.6627450980392157], [0.5411764705882353], [0.6431372549019608], [0.32941176470588235], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.4235294117647059], [0.5254901960784314], [0.5254901960784314], [0.7254901960784313], [0.32941176470588235], [0.28627450980392155], [0.4823529411764706], [0.3411764705882353], [0.47843137254901963], [0.43529411764705883], [0.4], [0.41568627450980394], [0.5019607843137255], [0.4470588235294118], [0.42745098039215684], [0.5254901960784314], [0.6823529411764706], [0.3803921568627451], [0.3843137254901961], [0.6274509803921569], [0.5764705882352941], [0.6862745098039216], [0.5294117647058824], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.3803921568627451], [0.5568627450980392], [0.6627450980392157], [0.7764705882352941], [0.1450980392156863], [0.32941176470588235], [0.4196078431372549], [0.3803921568627451], [0.47843137254901963], [0.4392156862745098], [0.42745098039215684], [0.4392156862745098], [0.49411764705882355], [0.4], [0.3764705882352941], [0.5137254901960784], [0.6745098039215687], [0.5019607843137255], [0.2], [0.996078431372549], [0.6588235294117647], [0.6431372549019608], [0.43529411764705883], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.047058823529411764], [0.1803921568627451], [0.00784313725490196], [0.4666666666666667], [0.4], [0.42745098039215684], [0.4823529411764706], [0.3764705882352941], [0.4549019607843137], [0.47843137254901963], [0.5176470588235295], [0.41568627450980394], [0.41568627450980394], [0.5058823529411764], [0.592156862745098], [0.7215686274509804], [0.10196078431372549], [0.0784313725490196], [0.03137254901960784], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.050980392156862744], [0.5372549019607843], [0.396078431372549], [0.4470588235294118], [0.39215686274509803], [0.41568627450980394], [0.5254901960784314], [0.5294117647058824], [0.5058823529411764], [0.40784313725490196], [0.43529411764705883], [0.4823529411764706], [0.592156862745098], [0.7607843137254902], [0.2901960784313726], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.011764705882352941], [0.0], [0.28627450980392155], [0.5176470588235295], [0.396078431372549], [0.40784313725490196], [0.4], [0.5490196078431373], [0.4235294117647059], [0.4235294117647059], [0.5137254901960784], [0.41568627450980394], [0.4666666666666667], [0.44313725490196076], [0.5568627450980392], [0.6549019607843137], [0.5294117647058824], [0.0], [0.00392156862745098], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.4392156862745098], [0.4627450980392157], [0.4196078431372549], [0.40784313725490196], [0.5450980392156862], [0.42745098039215684], [0.3803921568627451], [0.4823529411764706], [0.5411764705882353], [0.4196078431372549], [0.4980392156862745], [0.47058823529411764], [0.5333333333333333], [0.6313725490196078], [0.6235294117647059], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.011764705882352941], [0.0], [0.5568627450980392], [0.5803921568627451], [0.4392156862745098], [0.4117647058823529], [0.396078431372549], [0.3254901960784314], [0.49019607843137253], [0.4823529411764706], [0.5607843137254902], [0.40784313725490196], [0.45098039215686275], [0.39215686274509803], [0.49411764705882355], [0.6588235294117647], [0.6980392156862745], [0.027450980392156862], [0.0], [0.00784313725490196], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.00784313725490196], [0.0], [0.03529411764705882], [0.49411764705882355], [0.7215686274509804], [0.7843137254901961], [0.6549019607843137], [0.6392156862745098], [0.6705882352941176], [0.5882352941176471], [0.6549019607843137], [0.611764705882353], [0.6823529411764706], [0.7725490196078432], [0.7137254901960784], [0.6352941176470588], [0.23921568627450981], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.11764705882352941], [0.2823529411764706], [0.37254901960784315], [0.42745098039215684], [0.43529411764705883], [0.43529411764705883], [0.41568627450980394], [0.396078431372549], [0.2784313725490196], [0.047058823529411764], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]], [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.7529411764705882], [0.7372549019607844], [0.7098039215686275], [0.7411764705882353], [0.615686274509804], [0.6470588235294118], [0.7372549019607844], [0.6901960784313725], [0.7019607843137254], [0.7098039215686275], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.054901960784313725], [0.9215686274509803], [0.8352941176470589], [0.8352941176470589], [0.8745098039215686], [0.8823529411764706], [0.8509803921568627], [0.8392156862745098], [0.8], [0.8274509803921568], [0.7411764705882353], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.08627450980392157], [0.9176470588235294], [0.8509803921568627], [0.8274509803921568], [0.8509803921568627], [0.7843137254901961], [0.796078431372549], [0.8509803921568627], [0.796078431372549], [0.7019607843137254], [0.6666666666666666], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.4117647058823529], [0.9882352941176471], [0.803921568627451], [0.8196078431372549], [0.8588235294117647], [0.7568627450980392], [0.7647058823529411], [0.8313725490196079], [0.8117647058823529], [0.7607843137254902], [0.8], [0.2], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8], [0.8941176470588236], [0.7215686274509804], [0.807843137254902], [0.8431372549019608], [0.8666666666666667], [0.8313725490196079], [0.8196078431372549], [0.7803921568627451], [0.7450980392156863], [0.8901960784313725], [0.4117647058823529], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.9568627450980393], [0.7529411764705882], [0.7568627450980392], [0.8313725490196079], [0.8509803921568627], [0.8235294117647058], [0.8470588235294118], [0.8470588235294118], [0.7607843137254902], [0.6862745098039216], [0.8823529411764706], [0.5137254901960784], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.09803921568627451], [0.9686274509803922], [0.6666666666666666], [0.7529411764705882], [0.8235294117647058], [0.8274509803921568], [0.9372549019607843], [0.9058823529411765], [0.8588235294117647], [0.7803921568627451], [0.7019607843137254], [0.8509803921568627], [0.48627450980392156], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.19215686274509805], [0.9254901960784314], [0.6666666666666666], [0.7568627450980392], [0.788235294117647], [0.8627450980392157], [0.7333333333333333], [0.9058823529411765], [0.9019607843137255], [0.807843137254902], [0.7098039215686275], [0.8352941176470589], [0.4549019607843137], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.3411764705882353], [0.9098039215686274], [0.6235294117647059], [0.7568627450980392], [0.7843137254901961], [0.9607843137254902], [0.0], [0.8117647058823529], [0.9607843137254902], [0.7843137254901961], [0.6901960784313725], [0.8235294117647058], [0.42745098039215684], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.4980392156862745], [0.8666666666666667], [0.6666666666666666], [0.7411764705882353], [0.8470588235294118], [0.8823529411764706], [0.0], [0.592156862745098], [0.996078431372549], [0.7764705882352941], [0.7176470588235294], [0.8196078431372549], [0.403921568627451], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.6392156862745098], [0.807843137254902], [0.6901960784313725], [0.7254901960784313], [0.9490196078431372], [0.5725490196078431], [0.0], [0.20784313725490197], [1.0], [0.788235294117647], [0.7176470588235294], [0.803921568627451], [0.39215686274509803], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.6941176470588235], [0.7725490196078432], [0.6862745098039216], [0.7411764705882353], [0.9686274509803922], [0.13333333333333333], [0.0], [0.0], [0.9921568627450981], [0.796078431372549], [0.7058823529411765], [0.7803921568627451], [0.3803921568627451], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.6745098039215687], [0.7294117647058823], [0.6823529411764706], [0.7803921568627451], [0.8745098039215686], [0.0], [0.0], [0.0], [0.9176470588235294], [0.8], [0.6352941176470588], [0.7803921568627451], [0.41568627450980394], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.6039215686274509], [0.6941176470588235], [0.6823529411764706], [0.8274509803921568], [0.592156862745098], [0.0], [0.0], [0.0], [0.8235294117647058], [0.792156862745098], [0.5529411764705883], [0.7058823529411765], [0.4235294117647059], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.5529411764705883], [0.6705882352941176], [0.6862745098039216], [0.9647058823529412], [0.27450980392156865], [0.0], [0.00784313725490196], [0.0], [0.6941176470588235], [0.8], [0.4745098039215686], [0.5686274509803921], [0.5333333333333333], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.5294117647058824], [0.7019607843137254], [0.7490196078431373], [0.9725490196078431], [0.10980392156862745], [0.0], [0.0196078431372549], [0.0], [0.5254901960784314], [0.8235294117647058], [0.4196078431372549], [0.5568627450980392], [0.4666666666666667], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.35294117647058826], [0.7411764705882353], [0.7764705882352941], [0.8274509803921568], [0.01568627450980392], [0.0], [0.011764705882352941], [0.0], [0.23137254901960785], [0.9568627450980393], [0.4627450980392157], [0.6470588235294118], [0.4823529411764706], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.011764705882352941], [0.7333333333333333], [0.7647058823529411], [0.8823529411764706], [0.27450980392156865], [0.0], [0.01568627450980392], [0.0], [0.0784313725490196], [0.8431372549019608], [0.4549019607843137], [0.6549019607843137], [0.5372549019607843], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.7137254901960784], [0.7803921568627451], [0.8666666666666667], [0.6392156862745098], [0.0], [0.0], [0.0], [0.0], [0.9333333333333333], [0.5764705882352941], [0.7019607843137254], [0.5058823529411764], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.7607843137254902], [0.8156862745098039], [0.8352941176470589], [0.8509803921568627], [0.0], [0.0], [0.0], [0.0], [0.9647058823529412], [0.7725490196078432], [0.796078431372549], [0.5411764705882353], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.5294117647058824], [0.8823529411764706], [0.807843137254902], [0.788235294117647], [0.0], [0.0], [0.0], [0.0], [0.8431372549019608], [0.8431372549019608], [0.8666666666666667], [0.5490196078431373], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.10588235294117647], [0.8588235294117647], [0.8], [0.8509803921568627], [0.13333333333333333], [0.0], [0.0], [0.0], [0.792156862745098], [0.8431372549019608], [0.8784313725490196], [0.4823529411764706], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.796078431372549], [0.8392156862745098], [0.8745098039215686], [0.34509803921568627], [0.0], [0.0], [0.0], [0.7686274509803922], [0.8588235294117647], [0.9019607843137255], [0.5098039215686274], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.5215686274509804], [0.8509803921568627], [0.8509803921568627], [0.6549019607843137], [0.0], [0.0], [0.0], [0.7215686274509804], [0.8666666666666667], [0.9098039215686274], [0.5176470588235295], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.011764705882352941], [0.0], [0.23137254901960785], [0.8549019607843137], [0.8196078431372549], [0.7647058823529411], [0.0], [0.0], [0.0], [0.6627450980392157], [0.8941176470588236], [0.8392156862745098], [0.5882352941176471], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.7490196078431373], [0.807843137254902], [0.8588235294117647], [0.1843137254901961], [0.0], [0.0], [0.6078431372549019], [0.9294117647058824], [0.8745098039215686], [0.5215686274509804], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.6313725490196078], [0.9333333333333333], [0.9176470588235294], [0.4549019607843137], [0.0], [0.0], [0.6588235294117647], [0.9647058823529412], [0.9294117647058824], [0.611764705882353], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.14901960784313725], [0.5372549019607843], [0.5176470588235295], [0.20784313725490197], [0.0], [0.0], [0.3176470588235294], [0.5686274509803921], [0.5058823529411764], [0.24313725490196078], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]], [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.06666666666666667], [0.0], [0.13725490196078433], [0.21568627450980393], [0.20392156862745098], [0.17647058823529413], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.011764705882352941], [0.0], [0.00392156862745098], [0.9803921568627451], [1.0], [0.9607843137254902], [0.996078431372549], [0.9333333333333333], [0.9568627450980393], [0.9372549019607843], [0.5411764705882353], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.34509803921568627], [0.48627450980392156], [0.6666666666666666], [0.996078431372549], [0.5411764705882353], [0.7333333333333333], [1.0], [0.7333333333333333], [0.12549019607843137], [0.01568627450980392], [0.0], [0.00392156862745098], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.32941176470588235], [0.3843137254901961], [0.0], [0.7137254901960784], [0.8235294117647058], [0.9529411764705882], [1.0], [0.1450980392156863], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.00392156862745098], [0.0], [0.023529411764705882], [0.2196078431372549], [0.2823529411764706], [0.3568627450980392], [0.5215686274509804], [0.16862745098039217], [0.0], [0.9411764705882353], [0.8549019607843137], [0.0], [0.0], [0.15294117647058825], [0.1803921568627451], [0.0784313725490196], [0.09803921568627451], [0.00784313725490196], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.18823529411764706], [0.42745098039215684], [0.27450980392156865], [0.21176470588235294], [0.17254901960784313], [0.2784313725490196], [0.2196078431372549], [0.25098039215686274], [0.058823529411764705], [0.0784313725490196], [0.11372549019607843], [0.10980392156862745], [0.22745098039215686], [0.2235294117647059], [0.2], [0.0784313725490196], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.2549019607843137], [0.28627450980392155], [0.3215686274509804], [0.19215686274509805], [0.22745098039215686], [0.20392156862745098], [0.12549019607843137], [0.32941176470588235], [0.27058823529411763], [0.09803921568627451], [0.19607843137254902], [0.24705882352941178], [0.1803921568627451], [0.10588235294117647], [0.09803921568627451], [0.11372549019607843], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.047058823529411764], [0.3058823529411765], [0.20784313725490197], [0.5137254901960784], [0.1450980392156863], [0.2235294117647059], [0.20392156862745098], [0.0784313725490196], [0.35294117647058826], [0.3058823529411765], [0.0784313725490196], [0.20784313725490197], [0.24313725490196078], [0.1411764705882353], [0.06666666666666667], [0.10588235294117647], [0.15294117647058825], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.10980392156862745], [0.3333333333333333], [0.11372549019607843], [0.6039215686274509], [0.22745098039215686], [0.1843137254901961], [0.16862745098039217], [0.047058823529411764], [0.2980392156862745], [0.2784313725490196], [0.08235294117647059], [0.13333333333333333], [0.07450980392156863], [0.08235294117647059], [0.07450980392156863], [0.12941176470588237], [0.16862745098039217], [0.027450980392156862], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.16862745098039217], [0.30980392156862746], [0.050980392156862744], [0.5372549019607843], [0.2549019607843137], [0.1607843137254902], [0.16470588235294117], [0.0392156862745098], [0.32941176470588235], [0.2627450980392157], [0.050980392156862744], [0.11372549019607843], [0.10980392156862745], [0.07058823529411765], [0.1607843137254902], [0.17647058823529413], [0.11372549019607843], [0.08235294117647059], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.20784313725490197], [0.28627450980392155], [0.0392156862745098], [0.5725490196078431], [0.3333333333333333], [0.16862745098039217], [0.16470588235294117], [0.03529411764705882], [0.32941176470588235], [0.24705882352941178], [0.06274509803921569], [0.12156862745098039], [0.09411764705882353], [0.054901960784313725], [0.12941176470588237], [0.11372549019607843], [0.11372549019607843], [0.050980392156862744], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.23921568627450981], [0.27450980392156865], [0.00784313725490196], [0.6627450980392157], [0.4], [0.10980392156862745], [0.1843137254901961], [0.058823529411764705], [0.3137254901960784], [0.23529411764705882], [0.0392156862745098], [0.11372549019607843], [0.10196078431372549], [0.0], [0.30196078431372547], [0.10980392156862745], [0.10588235294117647], [0.054901960784313725], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.2627450980392157], [0.23529411764705882], [0.011764705882352941], [0.7176470588235294], [0.3058823529411765], [0.17254901960784313], [0.1803921568627451], [0.050980392156862744], [0.29411764705882354], [0.24313725490196078], [0.03529411764705882], [0.09411764705882353], [0.10980392156862745], [0.0], [0.6313725490196078], [0.15294117647058825], [0.050980392156862744], [0.08235294117647059], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.2901960784313726], [0.19607843137254902], [0.01568627450980392], [0.8705882352941177], [0.27450980392156865], [0.1450980392156863], [0.1803921568627451], [0.06274509803921569], [0.29411764705882354], [0.2549019607843137], [0.027450980392156862], [0.10196078431372549], [0.06274509803921569], [0.0], [0.9490196078431372], [0.1803921568627451], [0.027450980392156862], [0.09803921568627451], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.28627450980392155], [0.1411764705882353], [0.043137254901960784], [1.0], [0.2235294117647059], [0.17254901960784313], [0.21176470588235294], [0.043137254901960784], [0.2901960784313726], [0.24705882352941178], [0.01568627450980392], [0.10196078431372549], [0.023529411764705882], [0.00392156862745098], [0.8549019607843137], [0.28627450980392155], [0.0], [0.10588235294117647], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.03137254901960784], [0.29411764705882354], [0.11372549019607843], [0.09803921568627451], [1.0], [0.2627450980392157], [0.1803921568627451], [0.19607843137254902], [0.023529411764705882], [0.30980392156862746], [0.24705882352941178], [0.03137254901960784], [0.09803921568627451], [0.0], [0.10588235294117647], [0.9568627450980393], [0.396078431372549], [0.0], [0.11372549019607843], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.043137254901960784], [0.29411764705882354], [0.058823529411764705], [0.20784313725490197], [1.0], [0.22745098039215686], [0.15294117647058825], [0.19215686274509805], [0.07058823529411765], [0.2980392156862745], [0.2549019607843137], [0.023529411764705882], [0.10588235294117647], [0.01568627450980392], [0.0], [0.8627450980392157], [0.5411764705882353], [0.0], [0.10980392156862745], [0.011764705882352941], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.043137254901960784], [0.2901960784313726], [0.0196078431372549], [0.403921568627451], [0.996078431372549], [0.19215686274509805], [0.18823529411764706], [0.1803921568627451], [0.050980392156862744], [0.28627450980392155], [0.2549019607843137], [0.00784313725490196], [0.09803921568627451], [0.0196078431372549], [0.0], [0.8196078431372549], [0.6941176470588235], [0.0], [0.11764705882352941], [0.027450980392156862], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.06274509803921569], [0.29411764705882354], [0.01568627450980392], [0.4745098039215686], [1.0], [0.1411764705882353], [0.1843137254901961], [0.20392156862745098], [0.06274509803921569], [0.2627450980392157], [0.27058823529411763], [0.00784313725490196], [0.08627450980392157], [0.054901960784313725], [0.0], [0.7450980392156863], [0.7098039215686275], [0.0], [0.10980392156862745], [0.03137254901960784], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.08627450980392157], [0.3058823529411765], [0.0], [0.5098039215686274], [0.996078431372549], [0.08235294117647059], [0.23137254901960785], [0.22745098039215686], [0.10980392156862745], [0.2901960784313726], [0.2823529411764706], [0.00392156862745098], [0.10588235294117647], [0.09411764705882353], [0.0], [0.6862745098039216], [0.8], [0.0], [0.09411764705882353], [0.0392156862745098], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.09019607843137255], [0.30196078431372547], [0.0], [0.6078431372549019], [0.8549019607843137], [0.0784313725490196], [0.2235294117647059], [0.20784313725490197], [0.09411764705882353], [0.27450980392156865], [0.28627450980392155], [0.00784313725490196], [0.10588235294117647], [0.08627450980392157], [0.0], [0.5254901960784314], [0.8392156862745098], [0.0], [0.0784313725490196], [0.047058823529411764], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.09411764705882353], [0.29411764705882354], [0.0], [0.7254901960784313], [0.7450980392156863], [0.08235294117647059], [0.25098039215686274], [0.23137254901960785], [0.12941176470588237], [0.2823529411764706], [0.2823529411764706], [0.01568627450980392], [0.10196078431372549], [0.12156862745098039], [0.0], [0.47843137254901963], [0.8549019607843137], [0.011764705882352941], [0.06666666666666667], [0.06274509803921569], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.09411764705882353], [0.3176470588235294], [0.0], [0.7607843137254902], [0.615686274509804], [0.07058823529411765], [0.2235294117647059], [0.2196078431372549], [0.11764705882352941], [0.2784313725490196], [0.30196078431372547], [0.01568627450980392], [0.09019607843137255], [0.10196078431372549], [0.0], [0.43529411764705883], [0.8901960784313725], [0.050980392156862744], [0.050980392156862744], [0.07450980392156863], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.12549019607843137], [0.3058823529411765], [0.0], [0.8862745098039215], [0.5333333333333333], [0.2], [0.3215686274509804], [0.28627450980392155], [0.15294117647058825], [0.29411764705882354], [0.3137254901960784], [0.03137254901960784], [0.10980392156862745], [0.12941176470588237], [0.0], [0.4], [0.9490196078431372], [0.06274509803921569], [0.047058823529411764], [0.07450980392156863], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.1411764705882353], [0.27450980392156865], [0.011764705882352941], [0.9176470588235294], [0.32941176470588235], [0.20392156862745098], [0.29411764705882354], [0.29411764705882354], [0.2235294117647059], [0.25098039215686274], [0.25882352941176473], [0.07450980392156863], [0.1607843137254902], [0.15294117647058825], [0.03137254901960784], [0.20392156862745098], [0.8549019607843137], [0.17647058823529413], [0.023529411764705882], [0.06666666666666667], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.13725490196078433], [0.27058823529411763], [0.11372549019607843], [0.9411764705882353], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.8980392156862745], [0.43529411764705883], [0.0], [0.06666666666666667], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.25882352941176473], [0.32941176470588235], [0.17647058823529413], [0.45098039215686275], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.4666666666666667], [0.3058823529411765], [0.09411764705882353], [0.10196078431372549], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.21176470588235294], [0.2784313725490196], [0.12156862745098039], [0.2], [0.0], [0.0], [0.00392156862745098], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.1411764705882353], [0.11764705882352941], [0.10588235294117647], [0.10588235294117647], [0.0], [0.0], [0.0], [0.0]]]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q requests"
      ],
      "metadata": {
        "id": "R9zzo_-M-EuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "4dMGBgtK-ToB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show(idx, title):\n",
        "  plt.figure()\n",
        "  plt.imshow(X_test[idx].reshape(28,28))\n",
        "  plt.title('\\n\\n{}'.format(title), fontdict={'size': 16})"
      ],
      "metadata": {
        "id": "7LX9N2e1-4v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "headers = {\"content-type\": \"application/json\"}\n",
        "# 192.1.0.1\n",
        "json_response = requests.post('http://localhost:8501/v1/models/Fashion_MNIST_Model:predict', data=data, headers=headers)\n",
        "\n",
        "predictions = json.loads(json_response.text)['predictions']\n",
        "print(predictions)\n",
        "show(2,'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n",
        "  class_names[np.argmax(predictions[2])], y_test[2], class_names[np.argmax(predictions[2])], y_test[2]) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zgQWT-tw-UGA",
        "outputId": "04c150dc-d82f-4eae-c7f8-3db3cbe4c878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3.08734863e-13, 0.0, 1.28117687e-24, 8.42580346e-36, 4.42945424e-17, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.76290215e-11, 0.0, 1.0, 0.0, 2.54824327e-33, 0.0, 0.0, 0.0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# working break---> 10:22---> 10:37\n",
        "\n",
        "# this week---> individually spend time---> next week---> "
      ],
      "metadata": {
        "id": "jTvKTbiM-8gq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}